{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEnCQQRQIAC2ggj+QnXePF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/Fast_UNet/blob/main/akkadian2english_24feb2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pT3nQyhXvm2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uK-PjZFvKlb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    M2M100ForConditionalGeneration,\n",
        "    M2M100Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset, concatenate_datasets\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load all available data\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Acadian/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Acadian/test.csv\")\n",
        "published = pd.read_csv(\"/content/drive/MyDrive/Acadian/published_texts.csv\")\n",
        "lexicon = pd.read_csv(\"/content/drive/MyDrive/Acadian/OA_Lexicon_eBL.csv\")\n",
        "\n",
        "print(f\"Train: {len(train)}, Published: {len(published)}, Lexicon: {len(lexicon)}\")\n",
        "\n",
        "# ==================== IMPROVED PREPROCESSING ====================\n",
        "\n",
        "def advanced_normalize_transliteration(text):\n",
        "    \"\"\"Preserve important characters while cleaning\"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize Unicode\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "    # Preserve important Akkadian characters\n",
        "    # Don't replace these: ḫ, š, ṭ, ṣ, ḥ, â, ê, î, û, ā, ē, ī, ū\n",
        "\n",
        "    # Remove problematic characters but keep the important ones\n",
        "    text = re.sub(r\"[˹˺\\[\\]‹›«»]\", \"\", text)  # Remove scribal marks\n",
        "\n",
        "    # Handle hyphens consistently\n",
        "    text = text.replace(\"-\", \" \")  # Replace hyphens with spaces\n",
        "\n",
        "    # Normalize multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def augment_with_lexicon(lexicon_df, train_df):\n",
        "    \"\"\"Create additional training pairs from lexicon\"\"\"\n",
        "    augmented_pairs = []\n",
        "\n",
        "    for _, row in lexicon_df.iterrows():\n",
        "        if pd.notna(row.get('Akkadian', '')) and pd.notna(row.get('English', '')):\n",
        "            augmented_pairs.append({\n",
        "                'transliteration': advanced_normalize_transliteration(row['Akkadian']),\n",
        "                'translation': row['English']\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(augmented_pairs)\n",
        "\n",
        "def augment_with_published(published_df):\n",
        "    \"\"\"Extract sentence pairs from published texts\"\"\"\n",
        "    pairs = []\n",
        "\n",
        "    for _, row in published_df.iterrows():\n",
        "        if pd.notna(row.get('akkadian', '')) and pd.notna(row.get('translation', '')):\n",
        "            # Split into sentences for better alignment\n",
        "            akk_sentences = nltk.sent_tokenize(row['akkadian'])\n",
        "            eng_sentences = nltk.sent_tokenize(row['translation'])\n",
        "\n",
        "            # Simple alignment by sentence count\n",
        "            min_len = min(len(akk_sentences), len(eng_sentences))\n",
        "            for i in range(min_len):\n",
        "                pairs.append({\n",
        "                    'transliteration': advanced_normalize_transliteration(akk_sentences[i]),\n",
        "                    'translation': eng_sentences[i]\n",
        "                })\n",
        "\n",
        "    return pd.DataFrame(pairs)\n",
        "\n",
        "# Apply normalization to all datasets\n",
        "print(\"Normalizing training data...\")\n",
        "train['transliteration_norm'] = train['transliteration'].apply(advanced_normalize_transliteration)\n",
        "train['translation_norm'] = train['translation'].apply(lambda x: x.strip() if pd.notna(x) else \"\")\n",
        "\n",
        "# Create augmented data\n",
        "print(\"Creating augmented data...\")\n",
        "lexicon_pairs = augment_with_lexicon(lexicon, train)\n",
        "published_pairs = augment_with_published(published)\n",
        "\n",
        "# Combine all data\n",
        "all_data = pd.concat([\n",
        "    train[['transliteration_norm', 'translation_norm']].rename(\n",
        "        columns={'transliteration_norm': 'transliteration', 'translation_norm': 'translation'}\n",
        "    ),\n",
        "    lexicon_pairs,\n",
        "    published_pairs\n",
        "], ignore_index=True)\n",
        "\n",
        "# Remove duplicates and empty rows\n",
        "all_data = all_data.dropna(subset=['transliteration', 'translation'])\n",
        "all_data = all_data[all_data['transliteration'].str.len() > 0]\n",
        "all_data = all_data[all_data['translation'].str.len() > 0]\n",
        "all_data = all_data.drop_duplicates(subset=['transliteration'])\n",
        "\n",
        "print(f\"Total training pairs after augmentation: {len(all_data)}\")\n",
        "\n",
        "# Split into train/validation\n",
        "train_df, val_df = train_test_split(all_data, test_size=0.1, random_state=42)\n",
        "\n",
        "# ==================== MODEL SETUP ====================\n",
        "\n",
        "model_name = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# IMPORTANT: Use the correct language codes\n",
        "# M2M100 supports many languages - we'll use \"akk\" if available, otherwise \"fr\" as fallback\n",
        "# Check if Akkadian is supported\n",
        "try:\n",
        "    tokenizer.src_lang = \"akk\"  # Try Akkadian\n",
        "except:\n",
        "    tokenizer.src_lang = \"fr\"   # Fallback to French\n",
        "    print(\"Akkadian not in language list, using French as source\")\n",
        "\n",
        "tokenizer.tgt_lang = \"en\"\n",
        "\n",
        "# Add Akkadian-specific tokens\n",
        "special_tokens = {\n",
        "    \"additional_special_tokens\": [\n",
        "        \"ḫ\", \"š\", \"ṭ\", \"ṣ\", \"ḥ\", \"â\", \"ê\", \"î\", \"û\", \"ā\", \"ē\", \"ī\", \"ū\",\n",
        "        \"[\", \"]\", \"(\", \")\", \"{\", \"}\", \"<\", \">\"\n",
        "    ]\n",
        "}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# ==================== DATASET PREPARATION ====================\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"Tokenize with proper handling\"\"\"\n",
        "    # Source texts\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"transliteration\"],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    # Target texts\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"translation\"],\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=False\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Convert to datasets\n",
        "train_dataset = Dataset.from_pandas(train_df[['transliteration', 'translation']])\n",
        "val_dataset = Dataset.from_pandas(val_df[['transliteration', 'translation']])\n",
        "\n",
        "# Tokenize\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=['transliteration', 'translation']\n",
        ")\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=['transliteration', 'translation']\n",
        ")\n",
        "\n",
        "# ==================== TRAINING ARGUMENTS ====================\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    learning_rate=3e-5,  # Slightly higher learning rate\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=2,  # Simulate larger batch\n",
        "    num_train_epochs=50,\n",
        "    warmup_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=128,\n",
        "    generation_num_beams=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    save_total_limit=3,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    report_to=\"none\",\n",
        "    dataloader_num_workers=2,\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"polynomial\",\n",
        ")\n",
        "\n",
        "# ==================== TRAINER ====================\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "save_path = \"/content/drive/MyDrive/deep-past-model-improved\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")"
      ]
    }
  ]
}