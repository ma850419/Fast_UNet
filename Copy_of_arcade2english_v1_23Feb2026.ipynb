{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/Fast_UNet/blob/main/Copy_of_arcade2english_v1_23Feb2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EYv8NFfkrsQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXsHD9isPLDN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets sentencepiece sacrebleu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load core datasets\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Acadian/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Acadian/test.csv\")\n",
        "published = pd.read_csv(\"/content/drive/MyDrive/Acadian/published_texts.csv\")\n",
        "publications = pd.read_csv(\"/content/drive/MyDrive/Acadian/publications.csv\")\n",
        "lexicon = pd.read_csv(\"/content/drive/MyDrive/Acadian/OA_Lexicon_eBL.csv\")\n",
        "\n",
        "print(train.head())\n",
        "print(test.head())\n"
      ],
      "metadata": {
        "id": "Q1AjLO0iXwQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_transliteration(text):\n",
        "    # Remove scribal marks, normalize hyphens\n",
        "    text = re.sub(r\"['’]+\", \"\", text)\n",
        "    text = text.replace(\"-\", \" \")\n",
        "    return text.lower()\n",
        "\n",
        "train[\"translit_norm\"] = train[\"transliteration\"].apply(normalize_transliteration)\n",
        "test[\"translit_norm\"] = test[\"transliteration\"].apply(normalize_transliteration)\n"
      ],
      "metadata": {
        "id": "XDUoBl-WXx6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\") # <-- new requirement\n",
        "def split_sentences(text):\n",
        "    return nltk.sent_tokenize(text)\n",
        "\n",
        "train[\"translation_sentences\"] = train[\"translation\"].apply(split_sentences)\n"
      ],
      "metadata": {
        "id": "n-5C1NP6X3GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_data = []\n",
        "for _, row in train.iterrows():\n",
        "    for sent in row[\"translation_sentences\"]:\n",
        "        parallel_data.append((row[\"translit_norm\"], sent))\n",
        "\n",
        "parallel_df = pd.DataFrame(parallel_data, columns=[\"akkadian\", \"english\"])\n"
      ],
      "metadata": {
        "id": "8IMncBjNX7Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import M2M100Tokenizer\n",
        "\n",
        "model_name = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define source and target languages\n",
        "tokenizer.src_lang = \"en\"        # or \"fr\", \"de\", etc. depending on your source\n",
        "tokenizer.tgt_lang = \"en\"        # target is English\n"
      ],
      "metadata": {
        "id": "6TglJBGhsdTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Make sure the columns match what your preprocessing expects\n",
        "parallel_df = parallel_df.rename(columns={\n",
        "    \"akkadian\": \"transliteration\",\n",
        "    \"english\": \"translation\"\n",
        "})\n",
        "\n",
        "# Convert to Dataset\n",
        "dataset = Dataset.from_pandas(parallel_df[[\"transliteration\", \"translation\"]])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"transliteration\"], max_length=128, truncation=True\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        text_target=examples[\"translation\"], max_length=128, truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Map preprocessing\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "85qPEblqpPkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import transformers\n",
        "print(transformers.__version__)\n",
        "\n",
        "from transformers import (\n",
        "    M2M100ForConditionalGeneration,\n",
        "    M2M100Tokenizer,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Load datasets\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Acadian/train.csv\").dropna(subset=[\"transliteration\",\"translation\"])\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Acadian/test.csv\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(train[[\"transliteration\",\"translation\"]])\n",
        "\n",
        "# Choose multilingual base model\n",
        "model_name = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Add Akkadian transliteration characters if missing\n",
        "special_tokens = {\"additional_special_tokens\": [\"ḫ\", \"š\", \"ṭ\", \"ū\", \"ā\"]}\n",
        "tokenizer.add_special_tokens(special_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set source and target languages\n",
        "tokenizer.src_lang = \"fr\"   # treat Akkadian transliteration as \"fr\" (or another supported code)\n",
        "tokenizer.tgt_lang = \"en\"   # target is English\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples[\"transliteration\"],\n",
        "        max_length=128,\n",
        "        truncation=True\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        text_target=examples[\"translation\"],\n",
        "        max_length=128,\n",
        "        truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    do_eval=True,\n",
        "    eval_steps=200,                # still valid\n",
        "    logging_steps=50,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=20,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save model and tokenizer\n",
        "save_path = \"/content/drive/MyDrive/deep-past-model\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n"
      ],
      "metadata": {
        "id": "7sjWDr0kwCMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()   # upload kaggle.json here\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!mkdir my_dataset\n",
        "!cp -r /content/drive/MyDrive/deep-past-model/* my_dataset/\n"
      ],
      "metadata": {
        "id": "_vc1qDcbgm6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets create -p my_dataset"
      ],
      "metadata": {
        "id": "8xv-r13agrY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "wyhj_jnq2M7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def translate_texts(texts, model, tokenizer):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)   # ensure model is on GPU if available\n",
        "\n",
        "    # Tokenize and move inputs to the same device\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    # Generate translations\n",
        "    translated = model.generate(**inputs)\n",
        "\n",
        "    # Decode outputs\n",
        "    return [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "# Apply to your test set\n",
        "test[\"predicted_translation\"] = translate_texts(test[\"translit_norm\"].tolist(), model, tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "LsiMgNhAYStB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = test[[\"id\", \"predicted_translation\"]]\n",
        "submission.rename(columns={\"predicted_translation\": \"translation\"}, inplace=True)\n",
        "submission.to_csv(\"submission.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "er91YlPvYZOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}