{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee8d9ca9d46745189bf901cb5f29452f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a39a17f330db4a308282b23b53459ef3",
              "IPY_MODEL_cd3a2f37215640c9b065922c801d1b12",
              "IPY_MODEL_b3b4cfd079e84197aef35ada20630010"
            ],
            "layout": "IPY_MODEL_ea23069cac844128820b21d905298bf8"
          }
        },
        "a39a17f330db4a308282b23b53459ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a89a36bf62042f998602f3021138d31",
            "placeholder": "​",
            "style": "IPY_MODEL_c8ec5e1b07e64bbd91053609fc226941",
            "value": "model.safetensors: 100%"
          }
        },
        "cd3a2f37215640c9b065922c801d1b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7cae4df3b114115a7eb8a21356e1add",
            "max": 347452074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f51a5ae61a0c41e79ace58372d93c5da",
            "value": 347452074
          }
        },
        "b3b4cfd079e84197aef35ada20630010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e9e0060ff694476a160f9e1f62776cf",
            "placeholder": "​",
            "style": "IPY_MODEL_e5d06c10db6d410a961b5e53eb3b0ea8",
            "value": " 347M/347M [00:07&lt;00:00, 37.6MB/s]"
          }
        },
        "ea23069cac844128820b21d905298bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a89a36bf62042f998602f3021138d31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8ec5e1b07e64bbd91053609fc226941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7cae4df3b114115a7eb8a21356e1add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f51a5ae61a0c41e79ace58372d93c5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e9e0060ff694476a160f9e1f62776cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5d06c10db6d410a961b5e53eb3b0ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma850419/Fast_UNet/blob/main/Copy_of_csiro_pasture_06_A_jan2026_valbase16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EYv8NFfkrsQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d01ed91-53d9-48c5-999f-887b4d43d7eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kcFi9Gve3tKb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Data Loading and Exploration\n",
        "from pathlib import Path\n",
        "class PastureDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None, is_test=False):\n",
        "        \"\"\"\n",
        "        Dataset for pasture biomass prediction\n",
        "\n",
        "        Args:\n",
        "            csv_file: Path to csv file\n",
        "            image_dir: Directory with images\n",
        "            transform: Image transformations\n",
        "            is_test: Whether this is test data\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        # For training data, we need to handle multiple targets per image\n",
        "        if not is_test:\n",
        "            self.image_targets = self._prepare_training_data()\n",
        "\n",
        "    def _prepare_training_data(self):\n",
        "        \"\"\"Group targets by image for training\"\"\"\n",
        "        image_groups = self.data.groupby('image_path')\n",
        "        image_targets = {}\n",
        "\n",
        "        for image_path, group in image_groups:\n",
        "            targets = {}\n",
        "            for _, row in group.iterrows():\n",
        "                targets[row['target_name']] = row['target']\n",
        "            image_targets[image_path] = targets\n",
        "\n",
        "        return image_targets\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.is_test:\n",
        "            return len(self.data)\n",
        "        else:\n",
        "            return len(self.image_targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            sample = self.data.iloc[idx]\n",
        "            image_path = sample['image_path']  # already absolute\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            return {\n",
        "                'image': img,\n",
        "                'target_name': sample['target_name'],\n",
        "                'sample_id': sample['sample_id'],\n",
        "                'image_path': image_path\n",
        "            }\n",
        "        else:\n",
        "            image_path = list(self.image_targets.keys())[idx]\n",
        "            targets_dict = self.image_targets[image_path]\n",
        "\n",
        "            img = Image.open(image_path).convert('RGB')  # already absolute\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            target_order = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "            targets = [targets_dict.get(name, 0.0) for name in target_order]\n",
        "            targets = torch.FloatTensor(targets)\n",
        "\n",
        "            return {\n",
        "                'image': img,\n",
        "                'targets': targets,\n",
        "                'image_path': image_path\n",
        "            }\n",
        "\n",
        "\n",
        "def explore_data():\n",
        "    \"\"\"Explore the training data distribution\"\"\"\n",
        "    train_df = pd.read_csv(\"/content/drive/MyDrive/csiro/csiro-biomass/train.csv\")\n",
        "\n",
        "    print(\"Training data shape:\", train_df.shape)\n",
        "    print(\"\\nTarget value distribution:\")\n",
        "    print(train_df.groupby('target_name')['target'].describe())\n",
        "\n",
        "    # Plot target distributions\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "\n",
        "    for i, target in enumerate(targets):\n",
        "        target_data = train_df[train_df['target_name'] == target]['target']\n",
        "        axes[i].hist(target_data, bins=50, alpha=0.7)\n",
        "        axes[i].set_title(f'Distribution of {target}')\n",
        "        axes[i].set_xlabel('Biomass (g)')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Check for correlations between targets\n",
        "    pivot_df = train_df.pivot_table(index='sample_id', columns='target_name', values='target')\n",
        "    correlation_matrix = pivot_df.corr()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title('Correlation between Biomass Components')\n",
        "    plt.show()\n",
        "\n",
        "# Explore the data\n",
        "explore_data()"
      ],
      "metadata": {
        "id": "QOjmYkE23uGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PastureDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, target_mean=None, target_std=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "        self.target_mean = target_mean\n",
        "        self.target_std = target_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        targets = row[self.target_cols].values.astype(np.float32)\n",
        "        if self.target_mean is not None:\n",
        "            targets = (targets - self.target_mean) / self.target_std\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float32)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "pD0F2Di-t_uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vit-Base/16\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PastureBiomassModel(nn.Module):\n",
        "    def __init__(self, num_targets=5, pretrained=True):\n",
        "        super(PastureBiomassModel, self).__init__()\n",
        "\n",
        "        # -----------------------------\n",
        "        # ViT-Base/16 backbone (384x384)\n",
        "        # -----------------------------\n",
        "        self.backbone = timm.create_model(\n",
        "            \"vit_base_patch16_384\",\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0   # remove classifier, output features only\n",
        "        )\n",
        "\n",
        "        in_features = self.backbone.num_features  # 768 for ViT-B/16\n",
        "\n",
        "        # Shared MLP head\n",
        "        self.shared_features = nn.Sequential(\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        # Multi-target regression heads\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Linear(256, 1) for _ in range(num_targets)\n",
        "        ])\n",
        "\n",
        "        self.target_names = [\n",
        "            'Dry_Green_g',\n",
        "            'Dry_Dead_g',\n",
        "            'Dry_Clover_g',\n",
        "            'GDM_g',\n",
        "            'Dry_Total_g'\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, target_name=None):\n",
        "        features = self.backbone(x)  # (B, 768)\n",
        "        shared_out = self.shared_features(features)\n",
        "\n",
        "        if target_name is not None:\n",
        "            # Inference for a specific target\n",
        "            target_idx = self.target_names.index(target_name)\n",
        "            output = self.heads[target_idx](shared_out)\n",
        "            return output\n",
        "        else:\n",
        "            # Training: predict all 5 targets\n",
        "            outputs = [head(shared_out) for head in self.heads]\n",
        "            return torch.cat(outputs, dim=1)\n"
      ],
      "metadata": {
        "id": "LTYr56-Kk5Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTargetMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiTargetMSELoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # predictions: (B, 5)\n",
        "        # targets: (B, 5)\n",
        "        loss = 0\n",
        "        for i in range(targets.shape[1]):\n",
        "            loss += self.mse(predictions[:, i], targets[:, i])\n",
        "        return loss / targets.shape[1]\n"
      ],
      "metadata": {
        "id": "wUQhXIx-pKtq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset\n",
        "# -----------------------------\n",
        "class PastureDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, target_mean=None, target_std=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "        self.target_mean = target_mean\n",
        "        self.target_std = target_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        targets = row[self.target_cols].values.astype(np.float32)\n",
        "        # Normalize targets for training stability (these are in log space if log1p is used upstream)\n",
        "        if self.target_mean is not None and self.target_std is not None:\n",
        "            targets = (targets - self.target_mean) / self.target_std\n",
        "\n",
        "        return {\"image\": image, \"targets\": torch.tensor(targets, dtype=torch.float32)}\n",
        "\n",
        "# -----------------------------\n",
        "# Model: ViT backbone + shared MLP head + 5 linear heads\n",
        "# -----------------------------\n",
        "class PastureBiomassModel(nn.Module):\n",
        "    def __init__(self, num_targets=5, backbone_name=\"vit_base_patch16_384\", pretrained=True, head_width=1024):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0)\n",
        "        in_features = self.backbone.num_features  # typically 768 for ViT-B\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(in_features, head_width),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(head_width, head_width // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(head_width // 2, 256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.heads = nn.ModuleList([nn.Linear(256, 1) for _ in range(num_targets)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)           # (B, 768)\n",
        "        h = self.shared(feats)             # (B, 256)\n",
        "        outs = [head(h) for head in self.heads]\n",
        "        return torch.cat(outs, dim=1)      # (B, num_targets)\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "# -----------------------------\n",
        "# Training (modified)\n",
        "# -----------------------------\n",
        "def train_model(csv_path=\"/content/drive/MyDrive/csiro/csiro-biomass/train.csv\",\n",
        "                backbone_name=\"vit_base_patch16_384\",\n",
        "                input_size=384,\n",
        "                batch_size=16,\n",
        "                lr=1e-4,\n",
        "                weight_decay=1e-4,\n",
        "                warmup_freeze_epochs=3,   # earlier unfreezing\n",
        "                num_epochs=60,\n",
        "                use_loss_weights=True,\n",
        "                apply_log1p=True):\n",
        "\n",
        "    # 1) Transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # 2) Load and pivot\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df_wide = df.pivot_table(\n",
        "        index=[\"sample_id\", \"image_path\"],\n",
        "        columns=\"target_name\",\n",
        "        values=\"target\"\n",
        "    ).reset_index()\n",
        "    df_wide.columns.name = None\n",
        "\n",
        "    target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "\n",
        "    df_wide = df_wide.dropna(how=\"all\", subset=target_cols)\n",
        "    df_wide[target_cols] = df_wide[target_cols].fillna(0)\n",
        "\n",
        "    train_size = int(0.7 * len(df_wide))\n",
        "    train_df = df_wide.sample(n=train_size, random_state=42)\n",
        "    val_df = df_wide.drop(train_df.index)\n",
        "\n",
        "    if apply_log1p:\n",
        "        train_df[target_cols] = np.log1p(train_df[target_cols])\n",
        "        val_df[target_cols] = np.log1p(val_df[target_cols])\n",
        "\n",
        "    target_mean = train_df[target_cols].mean().values.astype(np.float32)\n",
        "    target_std = train_df[target_cols].std(ddof=0).values.astype(np.float32)\n",
        "    target_std = np.where(target_std < 1e-6, 1e-6, target_std)\n",
        "\n",
        "    # 4) Datasets & loaders\n",
        "    train_dataset = PastureDataset(train_df, transform=train_transform,\n",
        "                                   target_mean=target_mean, target_std=target_std)\n",
        "    val_dataset = PastureDataset(val_df, transform=val_transform,\n",
        "                                 target_mean=target_mean, target_std=target_std)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # 5) Model, loss, optimizer, scheduler\n",
        "    model = PastureBiomassModel(backbone_name=backbone_name, num_targets=5).to(device)\n",
        "\n",
        "    def set_backbone_requires_grad(flag):\n",
        "        for p in model.backbone.parameters():\n",
        "            p.requires_grad = flag\n",
        "\n",
        "    set_backbone_requires_grad(False)\n",
        "\n",
        "    # SmoothL1Loss (Huber)\n",
        "    base_criterion = nn.SmoothL1Loss(reduction='none')\n",
        "    loss_weights = torch.tensor(1.0 / target_std, dtype=torch.float32, device=device) if use_loss_weights else None\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'epoch # {epoch}')\n",
        "        if epoch == warmup_freeze_epochs:\n",
        "            set_backbone_requires_grad(True)\n",
        "\n",
        "        # ---------------- Train ----------------\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images = batch['image'].to(device, non_blocking=True)\n",
        "            targets = batch['targets'].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss_per_target = base_criterion(outputs, targets)\n",
        "            if loss_weights is not None:\n",
        "                loss_per_target = loss_per_target * loss_weights\n",
        "\n",
        "            loss = loss_per_target.mean()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        train_loss = running_train_loss / max(1, len(train_loader))\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}\")\n",
        "\n",
        "        # ---------------- Validate ----------------\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        all_preds, all_targets = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images = batch['image'].to(device, non_blocking=True)\n",
        "                targets = batch['targets'].to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(images)\n",
        "\n",
        "                val_loss_per_target = base_criterion(outputs, targets)\n",
        "                if loss_weights is not None:\n",
        "                    val_loss_per_target = val_loss_per_target * loss_weights\n",
        "                batch_val_loss = val_loss_per_target.mean().item()\n",
        "                running_val_loss += batch_val_loss\n",
        "\n",
        "                preds = outputs.cpu().numpy()\n",
        "                targets_np = targets.cpu().numpy()\n",
        "                preds = preds * target_std + target_mean\n",
        "                targets_np = targets_np * target_std + target_mean\n",
        "\n",
        "                if apply_log1p:\n",
        "                    preds = np.expm1(preds)\n",
        "                    targets_np = np.expm1(targets_np)\n",
        "\n",
        "                all_preds.append(preds)\n",
        "                all_targets.append(targets_np)\n",
        "\n",
        "        val_loss = running_val_loss / max(1, len(val_loader))\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_targets = np.vstack(all_targets)\n",
        "\n",
        "        for i, name in enumerate(target_cols):\n",
        "            r2_i = r2_score(all_targets[:, i], all_preds[:, i])\n",
        "            print(f\"{name}: R² = {r2_i:.4f}\")\n",
        "\n",
        "        r2_overall = r2_score(all_targets, all_preds, multioutput='uniform_average')\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]: Val Loss: {val_loss:.6f}, Overall R²: {r2_overall:.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss and not np.isnan(val_loss):\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_pasture_model.pth')\n",
        "            print(\"Saved best_pasture_model.pth\")\n",
        "\n",
        "\n",
        "    # ---------------- Plot ----------------\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training History')\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    model = train_model(\n",
        "        csv_path=\"/content/drive/MyDrive/csiro/csiro-biomass/train.csv\",\n",
        "        backbone_name=\"vit_base_patch16_384\",  # set to \"vit_base_patch16_384\" with input_size=384 if desired\n",
        "        input_size=384,\n",
        "        batch_size=16,\n",
        "        lr=1e-4,\n",
        "        weight_decay=1e-4,\n",
        "        warmup_freeze_epochs=10,\n",
        "        num_epochs=20,\n",
        "        use_loss_weights=True,\n",
        "        apply_log1p=True  # controls log scaling usage\n",
        "    )\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/csiro/vitbase_60epochs.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d1zvy5v1k_nn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ee8d9ca9d46745189bf901cb5f29452f",
            "a39a17f330db4a308282b23b53459ef3",
            "cd3a2f37215640c9b065922c801d1b12",
            "b3b4cfd079e84197aef35ada20630010",
            "ea23069cac844128820b21d905298bf8",
            "7a89a36bf62042f998602f3021138d31",
            "c8ec5e1b07e64bbd91053609fc226941",
            "f7cae4df3b114115a7eb8a21356e1add",
            "f51a5ae61a0c41e79ace58372d93c5da",
            "3e9e0060ff694476a160f9e1f62776cf",
            "e5d06c10db6d410a961b5e53eb3b0ea8"
          ]
        },
        "outputId": "af5aae03-094d-4b55-ef61-269502f558a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target mean: [0.5662349  0.39181235 0.23558907 0.6603299  0.7512836 ]\n",
            "Target std: [1.2482067  0.92493486 0.73027366 1.3471655  1.4974451 ]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/347M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee8d9ca9d46745189bf901cb5f29452f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch # 0\n",
            "Epoch [1/20], Train Loss: 1.017006\n",
            "Dry_Green_g: R² = -0.0833\n",
            "Dry_Dead_g: R² = -0.0793\n",
            "Dry_Clover_g: R² = -0.0210\n",
            "GDM_g: R² = -0.1053\n",
            "Dry_Total_g: R² = -0.1095\n",
            "Epoch [1/20]: Val Loss: 0.973020, Overall R²: -0.0797\n",
            "Saved best_pasture_model.pth\n",
            "epoch # 1\n",
            "Epoch [2/20], Train Loss: 0.915351\n",
            "Dry_Green_g: R² = -0.0779\n",
            "Dry_Dead_g: R² = -0.0763\n",
            "Dry_Clover_g: R² = -0.0269\n",
            "GDM_g: R² = -0.1006\n",
            "Dry_Total_g: R² = -0.1209\n",
            "Epoch [2/20]: Val Loss: 0.970823, Overall R²: -0.0805\n",
            "Saved best_pasture_model.pth\n",
            "epoch # 2\n",
            "Epoch [3/20], Train Loss: 0.917156\n",
            "Dry_Green_g: R² = -0.0751\n",
            "Dry_Dead_g: R² = -0.0746\n",
            "Dry_Clover_g: R² = -0.0169\n",
            "GDM_g: R² = -0.1005\n",
            "Dry_Total_g: R² = -0.1252\n",
            "Epoch [3/20]: Val Loss: 0.966207, Overall R²: -0.0785\n",
            "Saved best_pasture_model.pth\n",
            "epoch # 3\n",
            "Epoch [4/20], Train Loss: 0.909423\n",
            "Dry_Green_g: R² = -0.0841\n",
            "Dry_Dead_g: R² = -0.0865\n",
            "Dry_Clover_g: R² = 0.0029\n",
            "GDM_g: R² = -0.1062\n",
            "Dry_Total_g: R² = -0.1246\n",
            "Epoch [4/20]: Val Loss: 0.992462, Overall R²: -0.0797\n",
            "epoch # 4\n",
            "Epoch [5/20], Train Loss: 0.905723\n",
            "Dry_Green_g: R² = -0.0759\n",
            "Dry_Dead_g: R² = -0.0940\n",
            "Dry_Clover_g: R² = -0.0045\n",
            "GDM_g: R² = -0.1010\n",
            "Dry_Total_g: R² = -0.1247\n",
            "Epoch [5/20]: Val Loss: 0.983326, Overall R²: -0.0800\n",
            "epoch # 5\n",
            "Epoch [6/20], Train Loss: 0.909158\n",
            "Dry_Green_g: R² = -0.0726\n",
            "Dry_Dead_g: R² = -0.0825\n",
            "Dry_Clover_g: R² = -0.0009\n",
            "GDM_g: R² = -0.1008\n",
            "Dry_Total_g: R² = -0.1256\n",
            "Epoch [6/20]: Val Loss: 0.977797, Overall R²: -0.0765\n",
            "epoch # 6\n",
            "Epoch [7/20], Train Loss: 0.907862\n",
            "Dry_Green_g: R² = -0.0823\n",
            "Dry_Dead_g: R² = -0.0854\n",
            "Dry_Clover_g: R² = -0.0107\n",
            "GDM_g: R² = -0.1007\n",
            "Dry_Total_g: R² = -0.1164\n",
            "Epoch [7/20]: Val Loss: 0.978956, Overall R²: -0.0791\n",
            "epoch # 7\n",
            "Epoch [8/20], Train Loss: 0.903139\n",
            "Dry_Green_g: R² = -0.0757\n",
            "Dry_Dead_g: R² = -0.0816\n",
            "Dry_Clover_g: R² = -0.0036\n",
            "GDM_g: R² = -0.0947\n",
            "Dry_Total_g: R² = -0.1259\n",
            "Epoch [8/20]: Val Loss: 0.979373, Overall R²: -0.0763\n",
            "epoch # 8\n",
            "Epoch [9/20], Train Loss: 0.910519\n",
            "Dry_Green_g: R² = -0.0757\n",
            "Dry_Dead_g: R² = -0.0884\n",
            "Dry_Clover_g: R² = -0.0006\n",
            "GDM_g: R² = -0.0991\n",
            "Dry_Total_g: R² = -0.1246\n",
            "Epoch [9/20]: Val Loss: 0.986199, Overall R²: -0.0777\n",
            "epoch # 9\n",
            "Epoch [10/20], Train Loss: 0.893136\n",
            "Dry_Green_g: R² = -0.0756\n",
            "Dry_Dead_g: R² = -0.0882\n",
            "Dry_Clover_g: R² = 0.0002\n",
            "GDM_g: R² = -0.0994\n",
            "Dry_Total_g: R² = -0.1239\n",
            "Epoch [10/20]: Val Loss: 0.990224, Overall R²: -0.0774\n",
            "epoch # 10\n",
            "Epoch [11/20], Train Loss: 0.930027\n",
            "Dry_Green_g: R² = -0.0626\n",
            "Dry_Dead_g: R² = -0.0949\n",
            "Dry_Clover_g: R² = -0.0154\n",
            "GDM_g: R² = -0.1074\n",
            "Dry_Total_g: R² = -0.1265\n",
            "Epoch [11/20]: Val Loss: 0.979950, Overall R²: -0.0814\n",
            "epoch # 11\n",
            "Epoch [12/20], Train Loss: 0.929839\n",
            "Dry_Green_g: R² = -0.0635\n",
            "Dry_Dead_g: R² = -0.0747\n",
            "Dry_Clover_g: R² = -0.0314\n",
            "GDM_g: R² = -0.0968\n",
            "Dry_Total_g: R² = -0.1154\n",
            "Epoch [12/20]: Val Loss: 0.976814, Overall R²: -0.0764\n",
            "epoch # 12\n",
            "Epoch [13/20], Train Loss: 0.919928\n",
            "Dry_Green_g: R² = -0.0759\n",
            "Dry_Dead_g: R² = -0.0824\n",
            "Dry_Clover_g: R² = -0.0215\n",
            "GDM_g: R² = -0.1019\n",
            "Dry_Total_g: R² = -0.1214\n",
            "Epoch [13/20]: Val Loss: 0.964188, Overall R²: -0.0806\n",
            "Saved best_pasture_model.pth\n",
            "epoch # 13\n",
            "Epoch [14/20], Train Loss: 0.921242\n",
            "Dry_Green_g: R² = -0.0865\n",
            "Dry_Dead_g: R² = -0.0784\n",
            "Dry_Clover_g: R² = -0.0274\n",
            "GDM_g: R² = -0.1036\n",
            "Dry_Total_g: R² = -0.1180\n",
            "Epoch [14/20]: Val Loss: 0.966872, Overall R²: -0.0828\n",
            "epoch # 14\n",
            "Epoch [15/20], Train Loss: 0.914586\n",
            "Dry_Green_g: R² = -0.0755\n",
            "Dry_Dead_g: R² = -0.0806\n",
            "Dry_Clover_g: R² = -0.0263\n",
            "GDM_g: R² = -0.1068\n",
            "Dry_Total_g: R² = -0.1237\n",
            "Epoch [15/20]: Val Loss: 0.968090, Overall R²: -0.0826\n",
            "epoch # 15\n",
            "Epoch [16/20], Train Loss: 0.921130\n",
            "Dry_Green_g: R² = -0.0751\n",
            "Dry_Dead_g: R² = -0.0753\n",
            "Dry_Clover_g: R² = -0.0270\n",
            "GDM_g: R² = -0.1008\n",
            "Dry_Total_g: R² = -0.1196\n",
            "Epoch [16/20]: Val Loss: 0.970324, Overall R²: -0.0796\n",
            "epoch # 16\n",
            "Epoch [17/20], Train Loss: 0.937324\n",
            "Dry_Green_g: R² = -0.0846\n",
            "Dry_Dead_g: R² = -0.0803\n",
            "Dry_Clover_g: R² = -0.0198\n",
            "GDM_g: R² = -0.1063\n",
            "Dry_Total_g: R² = -0.1076\n",
            "Epoch [17/20]: Val Loss: 0.965406, Overall R²: -0.0797\n",
            "epoch # 17\n",
            "Epoch [18/20], Train Loss: 0.907102\n",
            "Dry_Green_g: R² = -0.0744\n",
            "Dry_Dead_g: R² = -0.0749\n",
            "Dry_Clover_g: R² = -0.0152\n",
            "GDM_g: R² = -0.1025\n",
            "Dry_Total_g: R² = -0.1170\n",
            "Epoch [18/20]: Val Loss: 0.968441, Overall R²: -0.0768\n",
            "epoch # 18\n",
            "Epoch [19/20], Train Loss: 0.912654\n",
            "Dry_Green_g: R² = -0.0738\n",
            "Dry_Dead_g: R² = -0.0750\n",
            "Dry_Clover_g: R² = -0.0186\n",
            "GDM_g: R² = -0.1026\n",
            "Dry_Total_g: R² = -0.1179\n",
            "Epoch [19/20]: Val Loss: 0.963647, Overall R²: -0.0776\n",
            "Saved best_pasture_model.pth\n",
            "epoch # 19\n",
            "Epoch [20/20], Train Loss: 0.898589\n",
            "Dry_Green_g: R² = -0.0710\n",
            "Dry_Dead_g: R² = -0.0727\n",
            "Dry_Clover_g: R² = -0.0159\n",
            "GDM_g: R² = -0.1019\n",
            "Dry_Total_g: R² = -0.1182\n",
            "Epoch [20/20]: Val Loss: 0.968146, Overall R²: -0.0759\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        images = batch['image'].to(device, non_blocking=True)\n",
        "        targets = batch['targets'].to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"NaN detected in validation loss, skipping batch\")\n",
        "            continue\n",
        "\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        # -----------------------------\n",
        "        # Denormalize predictions + targets for metrics\n",
        "        # -----------------------------\n",
        "        preds = outputs.cpu().numpy()\n",
        "        targets_np = targets.cpu().numpy()\n",
        "\n",
        "        preds = preds * target_std + target_mean\n",
        "        targets_np = targets_np * target_std + target_mean\n",
        "\n",
        "        all_preds.append(preds)\n",
        "        all_targets.append(targets_np)\n",
        "\n",
        "val_loss /= max(1, len(val_loader))\n",
        "train_losses.append(train_loss)\n",
        "val_losses.append(val_loss)\n",
        "\n",
        "scheduler.step(val_loss)\n",
        "\n",
        "print(f\"Epoch [{epoch+1}/{num_epochs}]: Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# After loop: compute R² in original units\n",
        "all_preds = np.vstack(all_preds)\n",
        "all_targets = np.vstack(all_targets)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(all_targets, all_preds)\n",
        "print(f\"Validation R²: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "DpeJotYvbML0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Multi-Target Regression Model\n",
        "class PastureBiomassModel(nn.Module):\n",
        "    def __init__(self, num_targets=5, pretrained=True):\n",
        "        super(PastureBiomassModel, self).__init__()\n",
        "\n",
        "        # Use EfficientNet as backbone\n",
        "        '''self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
        "\n",
        "        # Replace classifier\n",
        "        in_features = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Identity()  # Remove original classifier'''\n",
        "        self.backbone = models.resnet50(pretrained=pretrained)\n",
        "        # Replace final fully connected layer\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()  # Remove original head\n",
        "\n",
        "        # Multi-target regression heads\n",
        "        self.shared_features = nn.Sequential(\n",
        "            nn.Linear(in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Individual heads for each target\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Linear(256, 1) for _ in range(num_targets)\n",
        "        ])\n",
        "\n",
        "        self.target_names = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "\n",
        "    def forward(self, x, target_name=None):\n",
        "        features = self.backbone(x)\n",
        "        shared_out = self.shared_features(features)\n",
        "\n",
        "        if target_name is not None:\n",
        "            # For test time - specific target\n",
        "            target_idx = self.target_names.index(target_name)\n",
        "            output = self.heads[target_idx](shared_out)\n",
        "            return output\n",
        "        else:\n",
        "            # For training - all targets\n",
        "            outputs = [head(shared_out) for head in self.heads]\n",
        "            return torch.cat(outputs, dim=1)\n",
        "\n",
        "class MultiTargetMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiTargetMSELoss, self).__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        # Calculate MSE for each target and average\n",
        "        loss = 0\n",
        "        for i in range(targets.shape[1]):\n",
        "            loss += self.mse(predictions[:, i], targets[:, i])\n",
        "        return loss / targets.shape[1]"
      ],
      "metadata": {
        "id": "UdaxhoBA30Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Ensure your PastureBiomassModel (ViT-Base/16 backbone) is defined/imported\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset (expects a wide DataFrame)\n",
        "# -----------------------------\n",
        "class PastureDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, target_mean=None, target_std=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "        self.target_mean = target_mean\n",
        "        self.target_std = target_std\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        targets = row[self.target_cols].values.astype(np.float32)\n",
        "        # standardize targets\n",
        "        if self.target_mean is not None and self.target_std is not None:\n",
        "            targets = (targets - self.target_mean) / self.target_std\n",
        "\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    # -----------------------------\n",
        "    # 1. Data transformations (ViT normalization)\n",
        "    # -----------------------------\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.RandomResizedCrop(384, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(0.3),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # -----------------------------\n",
        "    # 2. Load CSV, pivot to wide format, clean, compute stats\n",
        "    # -----------------------------\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/csiro/csiro-biomass/train.csv\")\n",
        "\n",
        "    df_wide = df.pivot_table(\n",
        "        index=[\"sample_id\", \"image_path\"],\n",
        "        columns=\"target_name\",\n",
        "        values=\"target\"\n",
        "    ).reset_index()\n",
        "    df_wide.columns.name = None\n",
        "\n",
        "    target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "    df_wide[target_cols] = df_wide[target_cols].fillna(0)\n",
        "\n",
        "    target_mean = df_wide[target_cols].mean().values.astype(np.float32)\n",
        "    target_std = df_wide[target_cols].std(ddof=0).values.astype(np.float32)\n",
        "    target_std = np.where(target_std < 1e-6, 1e-6, target_std)\n",
        "\n",
        "    print(\"Target mean:\", target_mean)\n",
        "    print(\"Target std:\", target_std)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 3. Dataset and loaders\n",
        "    # -----------------------------\n",
        "    full_dataset = PastureDataset(\n",
        "        df_wide,\n",
        "        transform=train_transform,\n",
        "        target_mean=target_mean,\n",
        "        target_std=target_std\n",
        "    )\n",
        "\n",
        "    train_size = int(0.60 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    # apply val transform\n",
        "    val_dataset.dataset.transform = val_transform\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 4. Model, loss, optimizer, scheduler\n",
        "    # -----------------------------\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = PastureBiomassModel(num_targets=5).to(device)\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
        "\n",
        "    # -----------------------------\n",
        "    # 5. Training loop with gradient clipping and unified validation+R²\n",
        "    # -----------------------------\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    num_epochs = 25\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('epoch #', epoch)\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for step, batch in enumerate(train_loader, 1):\n",
        "            images = batch['image'].to(device, non_blocking=True)\n",
        "            targets = batch['targets'].to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            if torch.isnan(loss):\n",
        "                print(f\"NaN loss at step {step}, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= max(1, len(train_loader))\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        # -------- Validation (loss + R² in one pass) --------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                images = batch['image'].to(device, non_blocking=True)\n",
        "                targets = batch['targets'].to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "                if torch.isnan(loss):\n",
        "                    print(\"NaN detected in validation loss, skipping batch\")\n",
        "                    continue\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                all_preds.append(outputs.cpu().numpy())\n",
        "                all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "        val_loss /= max(1, len(val_loader))\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Compute R² (de-standardized)\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        all_targets = np.concatenate(all_targets, axis=0)\n",
        "        all_preds = all_preds * target_std + target_mean\n",
        "        all_targets = all_targets * target_std + target_mean\n",
        "\n",
        "        r2_scores = []\n",
        "        for i, col in enumerate(target_cols):\n",
        "            r2_scores.append(r2_score(all_targets[:, i], all_preds[:, i]))\n",
        "        mean_r2 = float(np.mean(r2_scores))\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}]: Val Loss: {val_loss:.4f}, Mean R²: {mean_r2:.4f}\")\n",
        "        print(\"Per-target R²:\", {col: round(r, 3) for col, r in zip(target_cols, r2_scores)})\n",
        "\n",
        "        # Step scheduler and save best\n",
        "        scheduler.step(val_loss)\n",
        "        if val_loss < best_val_loss and not np.isnan(val_loss):\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_pasture_model.pth')\n",
        "            print(\"Saved best_pasture_model.pth\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # 6. Plot training history\n",
        "    # -----------------------------\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training History')\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Run training\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    model = train_model()\n",
        "    torch.save(model.state_dict(), f\"/content/drive/MyDrive/csiro/vitbase_25epochs.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eXXJhlee36Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Prediction and Submission\n",
        "def create_submission():\n",
        "    \"\"\"Create submission file for test data\"\"\"\n",
        "\n",
        "    # Load test data\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/test.csv')\n",
        "    submission_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/sample_submission.csv')\n",
        "\n",
        "    # Load trained model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = PastureBiomassModel(num_targets=5)\n",
        "    model.load_state_dict(torch.load('best_pasture_model.pth', map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test transformations\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    full_dataset = PastureDataset(\n",
        "        \"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\",\n",
        "        image_dir=None)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    predictions = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            target_names = batch['target_name']\n",
        "            sample_ids = batch['sample_id']\n",
        "\n",
        "            for i, (sample_id, target_name) in enumerate(zip(sample_ids, target_names)):\n",
        "                # Get prediction for specific target\n",
        "                output = model(images[i].unsqueeze(0), target_name)\n",
        "                prediction = output.item()\n",
        "\n",
        "                # Ensure non-negative predictions\n",
        "                prediction = max(0, prediction)\n",
        "                predictions[sample_id] = prediction\n",
        "\n",
        "    # Create submission\n",
        "    submission_df['target'] = submission_df['sample_id'].map(predictions)\n",
        "\n",
        "    # Fill any missing values with median from training data\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/train.csv')\n",
        "    target_medians = train_df.groupby('target_name')['target'].median()\n",
        "\n",
        "    for sample_id in submission_df[submission_df['target'].isna()]['sample_id']:\n",
        "        target_name = test_df[test_df['sample_id'] == sample_id]['target_name'].iloc[0]\n",
        "        submission_df.loc[submission_df['sample_id'] == sample_id, 'target'] = target_medians[target_name]\n",
        "\n",
        "    # Save submission\n",
        "    submission_df.to_csv('/content/drive/MyDrive/csiro/submission.csv', index=False)\n",
        "    print(\"Submission file created: submission.csv\")\n",
        "\n",
        "    # Show prediction statistics\n",
        "    print(\"\\nPrediction Statistics:\")\n",
        "    print(submission_df['target'].describe())\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "# Create submission\n",
        "\n",
        "submission = create_submission()"
      ],
      "metadata": {
        "id": "I4233PLT3_6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacement of the above PastureData\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class PastureDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None, is_test=False, apply_log=True):\n",
        "        \"\"\"\n",
        "        Dataset for pasture biomass prediction.\n",
        "\n",
        "        Args:\n",
        "            csv_file: Path to csv file\n",
        "            image_dir: Directory with images\n",
        "            transform: Image transformations\n",
        "            is_test: Whether this is test data\n",
        "            apply_log: Apply log1p transform to targets (recommended)\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "        self.apply_log = apply_log\n",
        "\n",
        "        # For training data, group all targets belonging to the same image\n",
        "        if not is_test:\n",
        "            self.image_targets = self._prepare_training_data()\n",
        "\n",
        "        # Fixed target order required by the model\n",
        "        self.target_order = [\n",
        "            'Dry_Green_g',\n",
        "            'Dry_Dead_g',\n",
        "            'Dry_Clover_g',\n",
        "            'GDM_g',\n",
        "            'Dry_Total_g'\n",
        "        ]\n",
        "\n",
        "    def _prepare_training_data(self):\n",
        "        \"\"\"Group targets by image for training\"\"\"\n",
        "        image_groups = self.data.groupby('image_path')\n",
        "        image_targets = {}\n",
        "\n",
        "        for image_path, group in image_groups:\n",
        "            targets = {}\n",
        "            for _, row in group.iterrows():\n",
        "                targets[row['target_name']] = row['target']\n",
        "            image_targets[image_path] = targets\n",
        "\n",
        "        return image_targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) if self.is_test else len(self.image_targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_test:\n",
        "            # Test mode: return image + metadata only\n",
        "            sample = self.data.iloc[idx]\n",
        "            image_path = sample['image_path']\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            return {\n",
        "                'image': img,\n",
        "                'target_name': sample['target_name'],\n",
        "                'sample_id': sample['sample_id'],\n",
        "                'image_path': image_path\n",
        "            }\n",
        "\n",
        "        # Training mode\n",
        "        image_path = list(self.image_targets.keys())[idx]\n",
        "        targets_dict = self.image_targets[image_path]\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Extract targets in correct order\n",
        "        targets = np.array([targets_dict.get(name, 0.0) for name in self.target_order], dtype=float)\n",
        "\n",
        "        # ✅ Apply log1p transform (recommended for regression stability)\n",
        "        if self.apply_log:\n",
        "            targets = np.log1p(targets)\n",
        "\n",
        "        targets = torch.FloatTensor(targets)\n",
        "\n",
        "        return {\n",
        "            'image': img,\n",
        "            'targets': targets,\n",
        "            'image_path': image_path\n",
        "        }\n",
        "\n"
      ],
      "metadata": {
        "id": "bNDXgTVxQ3fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_submission():\n",
        "    \"\"\"Create submission file for test data\"\"\"\n",
        "\n",
        "    # Load test data\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/test.csv')\n",
        "    submission_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/sample_submission.csv')\n",
        "\n",
        "    # Load trained model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = PastureBiomassModel(num_targets=5)\n",
        "    model.load_state_dict(torch.load('best_pasture_model.pth', map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Test transformations\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # ✅ Correct dataset instantiation\n",
        "    test_dataset = PastureDataset(\n",
        "        \"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\",\n",
        "        image_dir=None,\n",
        "        transform=test_transform,\n",
        "        is_test=True\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "    predictions = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            target_names = batch['target_name']\n",
        "            sample_ids = batch['sample_id']\n",
        "\n",
        "            for i, (sample_id, target_name) in enumerate(zip(sample_ids, target_names)):\n",
        "                output = model(images[i].unsqueeze(0), target_name)\n",
        "                prediction = max(0, output.item())  # ensure non-negative\n",
        "                predictions[sample_id] = prediction\n",
        "\n",
        "    # Create submission\n",
        "    submission_df['target'] = submission_df['sample_id'].map(predictions)\n",
        "\n",
        "    # Fill missing values with medians\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/train.csv')\n",
        "    target_medians = train_df.groupby('target_name')['target'].median()\n",
        "\n",
        "    for sample_id in submission_df[submission_df['target'].isna()]['sample_id']:\n",
        "        target_name = test_df[test_df['sample_id'] == sample_id]['target_name'].iloc[0]\n",
        "        submission_df.loc[submission_df['sample_id'] == sample_id, 'target'] = target_medians[target_name]\n",
        "\n",
        "    # Save submission\n",
        "    submission_df.to_csv('/content/drive/MyDrive/csiro/submission.csv', index=False)\n",
        "    print(\"Submission file created: submission.csv\")\n",
        "\n",
        "    print(\"\\nPrediction Statistics:\")\n",
        "    print(submission_df['target'].describe())\n",
        "\n",
        "    return submission_df\n",
        "\n",
        "submission = create_submission()\n"
      ],
      "metadata": {
        "id": "VPzEtLfqjwHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTargetWrapper(nn.Module):\n",
        "    def __init__(self, backbone, in_features, num_targets=5):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.shared_features = nn.Sequential(\n",
        "            nn.Linear(in_features, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "        # Each head predicts a continuous biomass weight\n",
        "        self.heads = nn.ModuleList([nn.Linear(512, 1) for _ in range(num_targets)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        if x.dim() > 2:\n",
        "            x = torch.flatten(x, 1)\n",
        "        shared = self.shared_features(x)\n",
        "        outputs = [head(shared) for head in self.heads]\n",
        "        return torch.cat(outputs, dim=1)  # shape: (batch, num_targets)\n"
      ],
      "metadata": {
        "id": "tbMzI_htH-3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "model_paths = {\n",
        "    'mobilenet_v2': \"/content/drive/MyDrive/csiro/mobilenet_v2.pth\",\n",
        "    'googlenet': \"/content/drive/MyDrive/csiro/googlenet.pth\",\n",
        "    'alexnet': \"/content/drive/MyDrive/csiro/alexnet.pth\",\n",
        "    'resnet50': \"/content/drive/MyDrive/csiro/resnet50.pth\",\n",
        "    'convnext_tiny': \"/content/drive/MyDrive/csiro/convnext_tiny.pth\",\n",
        "    'efficientnet_b0': \"/content/drive/MyDrive/csiro/efficientnet_b0.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "def build_model(arch, num_targets=5, load_weights=False, device='cpu'):\n",
        "    if arch == 'mobilenet_v2':\n",
        "\n",
        "        backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # ✅ Extract correct feature size (1280)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "\n",
        "        # ✅ Remove classifier\n",
        "        backbone.classifier = nn.Identity()\n",
        "    elif arch == 'googlenet':\n",
        "        backbone = models.googlenet(weights=None, aux_logits=False)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "    elif arch == 'alexnet':\n",
        "        backbone = models.alexnet(weights=None)\n",
        "        # Flatten the conv output (256*6*6 = 9216)\n",
        "        in_features = 9216\n",
        "        backbone.classifier = nn.Sequential(\n",
        "            nn.Flatten()\n",
        "        )\n",
        "    elif arch == 'resnet50':\n",
        "        backbone = models.resnet50(weights=None)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "    elif arch == \"convnext_tiny\":\n",
        "        backbone = models.convnext_tiny(weights=None)\n",
        "        in_features = backbone.classifier[2].in_features\n",
        "        backbone.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "    elif arch == 'efficientnet_b0':\n",
        "        backbone = models.efficientnet_b0(weights=None)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture: {arch}\")\n",
        "\n",
        "    model = MultiTargetWrapper(backbone, in_features, num_targets=5)\n",
        "\n",
        "    if load_weights:\n",
        "        path = model_paths.get(arch)\n",
        "        if path:\n",
        "            state = torch.load(path, map_location=device)\n",
        "            model.load_state_dict(state, strict=True)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No path found for architecture: {arch}\")\n",
        "\n",
        "    return model'''\n"
      ],
      "metadata": {
        "id": "_Hv76CvKR6g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/ultralytics.git\n"
      ],
      "metadata": {
        "id": "JIAtpLi6zbpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n"
      ],
      "metadata": {
        "id": "jWiHRPJ-w3AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM_BiomassEstimator(nn.Module):\n",
        "    def __init__(self, backbone, feature_dim, hidden_dim=256, num_layers=1, num_targets=1):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "\n",
        "        # LSTM for temporal modeling\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=feature_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Regression head for biomass\n",
        "        self.fc = nn.Linear(hidden_dim, num_targets)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Accepts:\n",
        "            (B, C, H, W)  -> single image\n",
        "            (B, T, C, H, W) -> sequence of images\n",
        "        \"\"\"\n",
        "\n",
        "        # 🔥 Automatically convert single images into a sequence of length 1\n",
        "        if x.dim() == 4:\n",
        "            x = x.unsqueeze(1)   # (B, 1, C, H, W)\n",
        "\n",
        "        # Now safe to unpack\n",
        "        b, t, c, h, w = x.shape\n",
        "\n",
        "        # Merge batch + time for CNN\n",
        "        x = x.view(b * t, c, h, w)\n",
        "\n",
        "        # Extract CNN features\n",
        "        feats = self.backbone(x)  # (b*t, feature_dim)\n",
        "\n",
        "        # Restore sequence structure\n",
        "        feats = feats.view(b, t, -1)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(feats)\n",
        "\n",
        "        # Predict using last timestep\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "mFjGZa1yYudN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "# from ultralytics import YOLO   # uncomment if using YOLO\n",
        "\n",
        "model_paths = {\n",
        "    'mobilenet_v2': \"/content/drive/MyDrive/csiro/mobilenet_v2.pth\",\n",
        "    'googlenet': \"/content/drive/MyDrive/csiro/googlenet.pth\",\n",
        "    'alexnet': \"/content/drive/MyDrive/csiro/alexnet.pth\",\n",
        "    'resnet50': \"/content/drive/MyDrive/csiro/resnet50.pth\",\n",
        "    'convnext_tiny': \"/content/drive/MyDrive/csiro/convnext_tiny.pth\",\n",
        "    'efficientnet_b0': \"/content/drive/MyDrive/csiro/efficientnet_b0.pth\",\n",
        "    'yolov11': \"/content/drive/MyDrive/csiro/yolov11.pt\",\n",
        "    'yolov12': \"/content/drive/MyDrive/csiro/yolov12.pt\"\n",
        "}\n",
        "\n",
        "def build_model(\n",
        "    arch,\n",
        "    num_targets=5,\n",
        "    use_lstm=False,\n",
        "    hidden_dim=256,\n",
        "    num_layers=1,\n",
        "    load_weights=False,\n",
        "    device='cpu'\n",
        "):\n",
        "    # -----------------------------\n",
        "    # CNN BACKBONES\n",
        "    # -----------------------------\n",
        "    if arch == 'mobilenet_v2':\n",
        "        backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "        if use_lstm:\n",
        "            return CNN_LSTM_BiomassEstimator(\n",
        "                backbone=backbone,              # ✅ use backbone directly\n",
        "                feature_dim=in_features,        # ✅ use in_features\n",
        "                hidden_dim=hidden_dim,\n",
        "                num_layers=num_layers,\n",
        "                num_targets=num_targets\n",
        "            )\n",
        "\n",
        "    elif arch == 'googlenet':\n",
        "        backbone = models.googlenet(weights=None, aux_logits=False)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "\n",
        "    elif arch == 'alexnet':\n",
        "        backbone = models.alexnet(weights=None)\n",
        "        in_features = 9216\n",
        "        backbone.classifier = nn.Sequential(nn.Flatten())\n",
        "\n",
        "    elif arch == 'resnet50':\n",
        "        backbone = models.resnet50(weights=None)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "\n",
        "    elif arch == \"convnext_tiny\":\n",
        "        backbone = models.convnext_tiny(weights=None)\n",
        "        in_features = backbone.classifier[2].in_features\n",
        "        backbone.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "    elif arch == 'efficientnet_b0':\n",
        "        backbone = models.efficientnet_b0(weights=None)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "\n",
        "    # -----------------------------\n",
        "    # YOLO MODELS\n",
        "    # -----------------------------\n",
        "    elif arch in ['yolov11', 'yolov12']:\n",
        "        if load_weights:\n",
        "            model = YOLO(model_paths[arch])\n",
        "        else:\n",
        "            model = YOLO(f\"{arch}.yaml\")\n",
        "\n",
        "        # YOLO backbone feature dimension (example: 1024)\n",
        "        feature_dim = 1024  # adjust based on YOLO version\n",
        "\n",
        "        if use_lstm:\n",
        "            return CNN_LSTM_BiomassEstimator(\n",
        "                backbone=model.model.backbone,\n",
        "                feature_dim=feature_dim,\n",
        "                hidden_dim=hidden_dim,\n",
        "                num_layers=num_layers,\n",
        "                num_targets=num_targets\n",
        "            )\n",
        "        else:\n",
        "            return model\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture: {arch}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # APPLY LSTM OR STANDARD HEAD\n",
        "    # -----------------------------\n",
        "    if use_lstm:\n",
        "        model = CNN_LSTM_BiomassEstimator(\n",
        "            backbone=backbone,\n",
        "            feature_dim=in_features,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_targets=num_targets\n",
        "        )\n",
        "    else:\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "    # -----------------------------\n",
        "    # LOAD WEIGHTS IF REQUESTED\n",
        "    # -----------------------------\n",
        "    if load_weights and arch not in ['yolov11', 'yolov12']:\n",
        "        state = torch.load(model_paths[arch], map_location=device)\n",
        "        model.load_state_dict(state, strict=True)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "I8OCOOSZaF5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modified the above to include yolos\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "#from ultralytics import YOLO   # for YOLOv8, v9, v11, v12 and above\n",
        "\n",
        "model_paths = {\n",
        "    'mobilenet_v2': \"/content/drive/MyDrive/csiro/mobilenet_v2.pth\",\n",
        "    'googlenet': \"/content/drive/MyDrive/csiro/googlenet.pth\",\n",
        "    'alexnet': \"/content/drive/MyDrive/csiro/alexnet.pth\",\n",
        "    'resnet50': \"/content/drive/MyDrive/csiro/resnet50.pth\",\n",
        "    'convnext_tiny': \"/content/drive/MyDrive/csiro/convnext_tiny.pth\",\n",
        "    'efficientnet_b0': \"/content/drive/MyDrive/csiro/efficientnet_b0.pth\",\n",
        "    'yolov11': \"/content/drive/MyDrive/csiro/yolov11.pt\",\n",
        "    'yolov12': \"/content/drive/MyDrive/csiro/yolov12.pt\"\n",
        "}\n",
        "\n",
        "\n",
        "def build_model1(arch, num_targets=5, load_weights=False, device='cpu'):\n",
        "    if arch == 'mobilenet_v2':\n",
        "        backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "    else:\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "\n",
        "    elif arch == 'googlenet':\n",
        "        backbone = models.googlenet(weights=None, aux_logits=False)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "    elif arch == 'alexnet':\n",
        "        backbone = models.alexnet(weights=None)\n",
        "        in_features = 9216\n",
        "        backbone.classifier = nn.Sequential(nn.Flatten())\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "    elif arch == 'resnet50':\n",
        "        backbone = models.resnet50(weights=None)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "    elif arch == \"convnext_tiny\":\n",
        "        backbone = models.convnext_tiny(weights=None)\n",
        "        in_features = backbone.classifier[2].in_features\n",
        "        backbone.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "    elif arch == 'efficientnet_b0':\n",
        "        backbone = models.efficientnet_b0(weights=None)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "        model = MultiTargetWrapper(backbone, in_features, num_targets=num_targets)\n",
        "\n",
        "    elif arch in ['yolov11', 'yolov12']:\n",
        "        # YOLO models are detection networks, not classification backbones\n",
        "        if load_weights:\n",
        "            path = model_paths.get(arch)\n",
        "            if path:\n",
        "                model = YOLO(path)  # load pretrained weights\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"No path found for architecture: {arch}\")\n",
        "        else:\n",
        "            # Train from scratch using YAML config\n",
        "            model = YOLO(f\"{arch}.yaml\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture: {arch}\")\n",
        "\n",
        "    if load_weights and arch not in ['yolov11', 'yolov12']:\n",
        "        path = model_paths.get(arch)\n",
        "        if path:\n",
        "            state = torch.load(path, map_location=device)\n",
        "            model.load_state_dict(state, strict=True)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No path found for architecture: {arch}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "Mmj4Oc3oxwml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\")\n",
        "\n",
        "# Pivot so each image has one row with 5 targets\n",
        "wide_df = df.pivot(index=\"image_path\", columns=\"target_name\", values=\"sample_id\")\n",
        "\n",
        "# If you have actual numeric biomass values in another column (say \"value\"),\n",
        "# replace `sample_id` with that column instead.\n",
        "# Example:\n",
        "# wide_df = df.pivot(index=\"image_path\", columns=\"target_name\", values=\"value\")\n",
        "\n",
        "wide_df.reset_index(inplace=True)\n",
        "print(wide_df.head())\n"
      ],
      "metadata": {
        "id": "fCiVKLbHf49K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Test transformations\n",
        "\n",
        "class PastureDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        df = pd.read_csv(csv_file)\n",
        "        self.image_paths = df[\"image_path\"].unique()\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image  # only the image\n",
        "\n",
        "def predict_model(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:   # batch is a dict\n",
        "            images = batch.to(device)\n",
        "            outputs = model(images)\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "results = {}\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Correct dataset instantiation\n",
        "test_dataset = PastureDataset(\n",
        "    \"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\",\n",
        "    transform=test_transform\n",
        "\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "for arch, path in model_paths.items():\n",
        "    print(f\"\\nPredicting with {arch}...\")\n",
        "    model = build_model(arch, num_targets=5, load_weights=True, device=device)\n",
        "    preds = predict_model(model, test_loader, device)\n",
        "    results[arch] = preds\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\")\n",
        "combined = pd.DataFrame({\"image_path\": df[\"image_path\"].unique()})\n",
        "for arch, preds in results.items():\n",
        "    df = pd.DataFrame(preds, columns=[\n",
        "        f\"{arch}_Dry_Clover_g\", f\"{arch}_Dry_Dead_g\",\n",
        "        f\"{arch}_Dry_Green_g\", f\"{arch}_Dry_Total_g\", f\"{arch}_GDM_g\"\n",
        "    ])\n",
        "    combined = pd.concat([combined, df], axis=1)\n",
        "\n",
        "print(combined.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "MJBCHB-UjzUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "veg_types = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
        "\n",
        "records = []\n",
        "for img_path, pred in zip(test_dataset.image_paths, preds):\n",
        "    for veg, value in zip(veg_types, pred):\n",
        "        records.append({\"image_path\": img_path, \"vegetation\": veg, \"prediction\": value})\n",
        "\n",
        "pred_df = pd.DataFrame(records)\n"
      ],
      "metadata": {
        "id": "WLf5jyBnLuIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = torch.load(\"/content/drive/MyDrive/csiro/mobilenet_v2.pth\", map_location=\"cpu\")\n",
        "print([k for k in state.keys() if \"heads\" in k or \"shared_features\" in k])\n"
      ],
      "metadata": {
        "id": "lT9HHgQb5Ril"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Known class names from your CSV\n",
        "CLASS_NAMES = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    mae_sum, mse_sum, n = 0.0, 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            targets = batch['targets'].to(device).float()\n",
        "            outputs = model(images)\n",
        "\n",
        "            mae_sum += torch.mean(torch.abs(outputs - targets)).item()\n",
        "            mse_sum += torch.mean((outputs - targets) ** 2).item()\n",
        "            n += 1\n",
        "\n",
        "    return {\"MAE\": mae_sum/n, \"RMSE\": (mse_sum/n) ** 0.5}\n",
        "\n",
        "def recover_label_map(model, test_loader, device):\n",
        "    best_acc = -1.0\n",
        "    best_mapping = None\n",
        "\n",
        "    # Try all permutations of indices 0..4 assigned to the class names\n",
        "    for perm in itertools.permutations(range(len(CLASS_NAMES))):\n",
        "        candidate = {cls: idx for cls, idx in zip(CLASS_NAMES, perm)}\n",
        "        acc = evaluate_with_mapping(model, test_loader, candidate, device)\n",
        "        if acc > best_acc:\n",
        "            best_acc, best_mapping = acc, candidate\n",
        "\n",
        "    return best_mapping, best_acc\n",
        "\n",
        "# Example usage with one architecture (do for the most stable model first)\n",
        "print(\"\\nRecovering label_map using mobilenet_v2...\")\n",
        "model = build_model('mobilenet_v2', num_targets=5, load_weights=True, device=device)\n",
        "recovered_map, recovered_acc = recover_label_map(model, test_loader, device)\n",
        "print(\"Recovered mapping:\", recovered_map)\n",
        "print(f\"Accuracy with recovered mapping: {recovered_acc:.2%}\")\n"
      ],
      "metadata": {
        "id": "UtbaqCBtvMkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "# Image preprocessing\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),   # resize to match backbone input\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Train dataset\n",
        "train_dataset = PastureDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/csiro/csiro-biomass/train.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/csiro/csiro-biomass/train\",\n",
        "    transform=train_transforms,\n",
        "    is_test=False\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_dataset = PastureDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/csiro/csiro-biomass/test\",\n",
        "    transform=test_transforms,\n",
        "    is_test=True\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)'''\n"
      ],
      "metadata": {
        "id": "6SK8zgALaxfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified codes of the above\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. Train/Validation Split\n",
        "# ---------------------------------------------------------\n",
        "full_df = pd.read_csv(\"/content/drive/MyDrive/csiro/csiro-biomass/train.csv\")\n",
        "\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.15, random_state=42)\n",
        "\n",
        "train_df.to_csv(\"train_split.csv\", index=False)\n",
        "val_df.to_csv(\"val_split.csv\", index=False)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. Image preprocessing (KEEPING 384×384)\n",
        "# ---------------------------------------------------------\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),     # ✅ keep original competition size\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),     # ✅ keep original size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. Datasets (log1p enabled)\n",
        "# ---------------------------------------------------------\n",
        "train_dataset = PastureDataset(\n",
        "    csv_file=\"train_split.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/csiro/csiro-biomass/train\",\n",
        "    transform=train_transforms,\n",
        "    is_test=False,\n",
        "    apply_log=True\n",
        ")\n",
        "\n",
        "val_dataset = PastureDataset(\n",
        "    csv_file=\"val_split.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/csiro/csiro-biomass/train\",\n",
        "    transform=test_transforms,\n",
        "    is_test=False,\n",
        "    apply_log=True\n",
        ")\n",
        "\n",
        "test_dataset = PastureDataset(\n",
        "    csv_file=\"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\",\n",
        "    image_dir=\"/content/drive/MyDrive/csiro/csiro-biomass/test\",\n",
        "    transform=test_transforms,\n",
        "    is_test=True,\n",
        "    apply_log=False\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. DataLoaders\n",
        "# ---------------------------------------------------------\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "qPedX4BDXcs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_backbone_output_dim(backbone):\n",
        "    dummy = torch.randn(1, 3, 384, 384)   # standard input size\n",
        "    with torch.no_grad():\n",
        "        out = backbone(dummy)\n",
        "        if out.ndim > 2:                  # flatten if conv features\n",
        "            out = torch.flatten(out, 1)\n",
        "    return out.shape[1]\n",
        "def build_model(arch, num_targets=5):\n",
        "    if arch == 'mobilenet_v2':\n",
        "        backbone = models.mobilenet_v2(weights=None)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "\n",
        "    elif arch == 'googlenet':\n",
        "        backbone = models.googlenet(weights=None, aux_logits=False)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "\n",
        "    elif arch == 'alexnet':\n",
        "        backbone = models.alexnet(weights=None)\n",
        "        backbone.classifier = nn.Identity()\n",
        "        in_features = 9216  # correct feature size\n",
        "    elif arch == 'resnet50':\n",
        "        backbone = models.resnet50(weights=None)\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Identity()\n",
        "\n",
        "    elif arch == 'convnext_tiny':\n",
        "        backbone = models.convnext_tiny(weights=None)\n",
        "        in_features = backbone.classifier[2].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "\n",
        "    elif arch == 'efficientnet_b0':\n",
        "        backbone = models.efficientnet_b0(weights=None)\n",
        "        in_features = backbone.classifier[1].in_features\n",
        "        backbone.classifier = nn.Identity()\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported architecture: {arch}\")\n",
        "\n",
        "    in_features = get_backbone_output_dim(backbone)\n",
        "    # Shared regression head\n",
        "    shared_features = nn.Sequential(\n",
        "        nn.Linear(in_features, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2)\n",
        "    )\n",
        "\n",
        "    heads = nn.ModuleList([nn.Linear(256, 1) for _ in range(num_targets)])\n",
        "\n",
        "    return MultiTargetWrapper(backbone, in_features, num_targets=5)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uin4vNHdbOMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch.optim as optim\n",
        "def eval_r2(model, val_loader, device):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            y = batch['targets'].numpy()\n",
        "            out = model(images).cpu().numpy()\n",
        "            preds.append(out)\n",
        "            targets.append(y)\n",
        "    preds = np.vstack(preds)\n",
        "    targets = np.vstack(targets)\n",
        "    return [r2_score(targets[:,i], preds[:,i]) for i in range(5)]\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, device, num_epochs=10, lr=1e-4):\n",
        "    #criterion = nn.MSELoss()\n",
        "    weights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5], device=device)\n",
        "    def weighted_mse(pred, target):\n",
        "        return ((pred - target)**2 * weights).mean()\n",
        "    criterion = weighted_mse\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            targets = batch['targets'].to(device).float()  # continuous values\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)  # (batch, 5)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, MSE Loss: {avg_loss:.4f}\")\n",
        "    print(eval_r2(model, val_loader, device))\n",
        "    return model'''\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "muGKKWK_bPav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modified above codes\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def eval_r2(model, val_loader, device):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            y = batch['targets'].numpy()\n",
        "            out = model(images).cpu().numpy()\n",
        "            preds.append(out)\n",
        "            targets.append(y)\n",
        "    preds = np.vstack(preds)\n",
        "    targets = np.vstack(targets)\n",
        "    return [r2_score(targets[:,i], preds[:,i]) for i in range(5)]\n",
        "def train_model(model, train_loader, val_loader, device, num_epochs=30, lr=7e-8):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            targets = batch['targets'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, MSE Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # ✅ Evaluate R²\n",
        "        r2 = eval_r2(model, val_loader, device)\n",
        "        print(\"Validation R²:\", r2)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "TKYqp9zoXwfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "architectures = [\n",
        "   # 'efficientnet_b0',\n",
        "   'mobilenet_v2',\n",
        "   # 'googlenet',\n",
        "   # 'alexnet',\n",
        "    #'resnet50',\n",
        "   # 'convnext_tiny'\n",
        "]\n",
        "trained_models = {}\n",
        "\n",
        "for arch in architectures:\n",
        "    print(f\"\\nTraining {arch} from scratch...\")\n",
        "    model = build_model(arch, num_targets=5,use_lstm=True)\n",
        "    #trained_model = train_model(model, train_loader, device, num_epochs=20, lr=1e-4)\n",
        "    trained_model = train_model(model, train_loader, val_loader, device, num_epochs=50, lr=5e-7)\n",
        "    trained_models[arch] = trained_model\n",
        "    # Save weights\n",
        "    torch.save(trained_model.state_dict(), f\"/content/drive/MyDrive/csiro/{arch}_lstm_50_.pth\")\n"
      ],
      "metadata": {
        "id": "x-TzQCPGbTFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet50 + LSTM\n",
        "class ResNetLSTM(nn.Module):\n",
        "    def __init__(self, num_targets=5, hidden_size=256, num_layers=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.backbone = models.resnet50(weights=None)\n",
        "        feat_dim = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Identity()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=feat_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(lstm_out_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_targets)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, C, H, W)\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = x.view(b * t, c, h, w)\n",
        "        feats = self.backbone(x)         # (b*t, feat_dim)\n",
        "        if feats.dim() > 2:\n",
        "            feats = torch.flatten(feats, 1)\n",
        "        feats = feats.view(b, t, -1)\n",
        "        lstm_out, _ = self.lstm(feats)\n",
        "        last_out = lstm_out[:, -1, :]\n",
        "        out = self.head(last_out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "kEQbnb78grlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mobilenet+LSTM\n",
        "\n",
        "import torchvision.models as models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class MobileNetLSTM(nn.Module):\n",
        "    def __init__(self, num_targets=5, hidden_size=256, num_layers=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.backbone = models.mobilenet_v2(weights=None)   # start from scratch\n",
        "        feat_dim = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=feat_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        lstm_out_dim = hidden_size * (2 if bidirectional else 1)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(lstm_out_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_targets)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Expecting (batch, seq_len, C, H, W)\n",
        "        if x.dim() == 4:\n",
        "            # If dataset gives single images, add seq_len=1\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = x.view(b * t, c, h, w)\n",
        "        feats = self.backbone(x)\n",
        "        if feats.dim() > 2:\n",
        "            feats = torch.flatten(feats, 1)\n",
        "\n",
        "        feats = feats.view(b, t, -1)\n",
        "        lstm_out, _ = self.lstm(feats)\n",
        "        last_out = lstm_out[:, -1, :]\n",
        "        out = self.head(last_out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "XvCvOYQHlJ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model\n",
        "model = MobileNetLSTM(num_targets=5, hidden_size=256, num_layers=1, bidirectional=False)\n",
        "\n",
        "# Train model\n",
        "trained_model = train_model(model, train_loader, val_loader, device, num_epochs=30, lr=1e-4)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# Save the trained model\n",
        "# ---------------------------------------------------------\n",
        "# Option 1: Save only the state_dict (recommended)\n",
        "torch.save(trained_model.state_dict(), \"mobilenet_lstm_model.pth\")\n",
        "\n",
        "# Option 2: Save the entire model (less portable)\n",
        "# torch.save(trained_model, \"mobilenet_lstm_full.pth\")\n"
      ],
      "metadata": {
        "id": "iujWNVaXlNnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joint MobileNetV2 + ResNet50 model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class DualBackboneMultiTarget(nn.Module):\n",
        "    def __init__(self, num_targets=5):\n",
        "        super().__init__()\n",
        "\n",
        "        # MobileNetV2 backbone\n",
        "        self.mobilenet = models.mobilenet_v2(weights=None)\n",
        "        mobilenet_features = self.mobilenet.classifier[1].in_features\n",
        "        self.mobilenet.classifier = nn.Identity()\n",
        "\n",
        "        # ResNet50 backbone\n",
        "        self.resnet = models.resnet50(weights=None)\n",
        "        resnet_features = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        # Total fused feature size\n",
        "        fused_in_features = mobilenet_features + resnet_features\n",
        "\n",
        "        # Shared MLP (similar to your MultiTargetWrapper)\n",
        "        self.shared_features = nn.Sequential(\n",
        "            nn.Linear(fused_in_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2)\n",
        "        )\n",
        "\n",
        "        # Multi‑target heads\n",
        "        self.heads = nn.ModuleList([nn.Linear(256, 1) for _ in range(num_targets)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat_m = self.mobilenet(x)  # (batch, mobilenet_features)\n",
        "        feat_r = self.resnet(x)     # (batch, resnet_features)\n",
        "\n",
        "        # If any is not flattened (should already be), ensure flatten:\n",
        "        if feat_m.dim() > 2:\n",
        "            feat_m = torch.flatten(feat_m, 1)\n",
        "        if feat_r.dim() > 2:\n",
        "            feat_r = torch.flatten(feat_r, 1)\n",
        "\n",
        "        fused = torch.cat([feat_m, feat_r], dim=1)  # (batch, fused_in_features)\n",
        "        shared = self.shared_features(fused)\n",
        "        outputs = [head(shared) for head in self.heads]\n",
        "        return torch.cat(outputs, dim=1)  # (batch, num_targets)\n",
        "\n",
        "# Build and optionally load individual backbone weights\n",
        "def build_dual_model(mobilenet_path=None, resnet_path=None, device='cpu', num_targets=5):\n",
        "    model = DualBackboneMultiTarget(num_targets=num_targets)\n",
        "\n",
        "    # Optional: initialize each backbone with your pre‑trained weights\n",
        "    if mobilenet_path is not None:\n",
        "        single_mobilenet = build_model('mobilenet_v2', num_targets=num_targets, load_weights=True, device=device)\n",
        "        model.mobilenet.load_state_dict(single_mobilenet.backbone.state_dict(), strict=False)\n",
        "\n",
        "    if resnet_path is not None:\n",
        "        single_resnet = build_model('resnet50', num_targets=num_targets, load_weights=True, device=device)\n",
        "        model.resnet.load_state_dict(single_resnet.backbone.state_dict(), strict=False)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dual_model = build_dual_model(\n",
        "    mobilenet_path=\"/content/drive/MyDrive/csiro/mobilenet_v2.pth\",\n",
        "    resnet_path=\"/content/drive/MyDrive/csiro/resnet50.pth\",\n",
        "    device=device,\n",
        "    num_targets=5\n",
        ").to(device)\n",
        "\n",
        "def train_model_regression(model, train_loader, device, num_epochs=10, lr=1e-4):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            targets = batch['targets'].to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, MSE Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the dual model\n",
        "dual_model = train_model_regression(dual_model, train_loader, device, num_epochs=30, lr=1e-4)\n",
        "torch.save(dual_model.state_dict(), \"/content/drive/MyDrive/csiro/mobilenet_resnet_dual.pth\")\n"
      ],
      "metadata": {
        "id": "ZKhpkoLdhOEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading models after training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#/content/drive/MyDrive/csiro/efficientnet_b0.pth\n",
        "architectures = [\n",
        "    'efficientnet_b0',\n",
        "    'mobilenet_v2',\n",
        "    'googlenet',\n",
        "    'alexnet',\n",
        "    'resnet50',\n",
        "    'convnext_tiny'\n",
        "]\n",
        "import torch\n",
        "\n",
        "loaded_models = {}\n",
        "\n",
        "for arch in architectures:\n",
        "    print(f\"Loading {arch}...\")\n",
        "    model = build_model(arch, num_targets=5)   # must match training setup\n",
        "    model.load_state_dict(torch.load(f\"/content/drive/MyDrive/csiro/{arch}.pth\", map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    loaded_models[arch] = model\n"
      ],
      "metadata": {
        "id": "gmr83SRn76NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to use with testing only\n",
        "class PastureDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return {\n",
        "            \"image\": image,\n",
        "            \"sample_id\": row[\"sample_id\"]\n",
        "        }"
      ],
      "metadata": {
        "id": "NmEHc_zrCVYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing models after training\n",
        "import pandas as pd\n",
        "\n",
        "# Target names in fixed order (must match training order)\n",
        "target_names = [\"Dry_Clover_g\", \"Dry_Dead_g\", \"Dry_Green_g\", \"Dry_Total_g\", \"GDM_g\"]\n",
        "\n",
        "def generate_submission(model, test_loader, device, output_csv=\"submission.csv\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch['image'].to(device)\n",
        "            outputs = model(images)  # shape (batch, 5)\n",
        "            outputs = outputs.cpu().numpy()\n",
        "\n",
        "            # Each row in test.csv has a sample_id like IDxxxx__TargetName\n",
        "            sample_ids = batch['sample_id']\n",
        "\n",
        "            for i, sid in enumerate(sample_ids):\n",
        "                target_name = sid.split(\"__\")[1]\n",
        "                idx = target_names.index(target_name)\n",
        "\n",
        "                predictions.append({\n",
        "                    \"sample_id\": sid,\n",
        "                    \"target\": float(outputs[i, idx])\n",
        "                })\n",
        "\n",
        "    df = pd.DataFrame(predictions)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"Saved predictions to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "ZtJlqU6z6jRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Looping over all models\n",
        "# Define transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Dataset and loader\n",
        "test_dataset = PastureDataset(\n",
        "    \"/content/drive/MyDrive/csiro/csiro-biomass/test.csv\",\n",
        "    transform=test_transform\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "# Run submission generation\n",
        "for arch, model in loaded_models.items():\n",
        "    print(f\"Generating submission for {arch}...\")\n",
        "    generate_submission(model, test_loader, device,\n",
        "                        output_csv=f\"/content/drive/MyDrive/csiro/{arch}_submission.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9A5ZSw6V8dcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Right after you create train_loader\n",
        "batch = next(iter(train_loader))\n",
        "print(\"Sample targets:\", batch['targets'][:10])\n",
        "print(\"Unique values:\", torch.unique(batch['targets']))\n"
      ],
      "metadata": {
        "id": "QBAcPixVHamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Data Analysis and Feature Engineering\n",
        "def additional_analysis():\n",
        "    \"\"\"Additional data analysis for insights\"\"\"\n",
        "\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/csiro/csiro-biomass/train.csv')\n",
        "\n",
        "    # Analyze by state\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i, target in enumerate(['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g'], 1):\n",
        "        plt.subplot(2, 3, i)\n",
        "        target_data = train_df[train_df['target_name'] == target]\n",
        "        sns.boxplot(data=target_data, x='State', y='target')\n",
        "        plt.title(f'{target} by State')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Analyze relationship with NDVI and Height\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    targets = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
        "\n",
        "    for i, target in enumerate(targets):\n",
        "        target_data = train_df[train_df['target_name'] == target]\n",
        "        axes[i].scatter(target_data['Pre_GSHH_NDVI'], target_data['target'], alpha=0.5)\n",
        "        axes[i].set_xlabel('NDVI')\n",
        "        axes[i].set_ylabel(target)\n",
        "        axes[i].set_title(f'{target} vs NDVI')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run additional analysis\n",
        "additional_analysis()"
      ],
      "metadata": {
        "id": "JwGjIBtZ4LiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Key Features of This Solution:\n",
        "Multi-Target Architecture: Single model predicting all 5 biomass components\n",
        "\n",
        "Data Augmentation: Improved generalization with image transformations\n",
        "\n",
        "Transfer Learning: Using pre-trained EfficientNet backbone\n",
        "\n",
        "Comprehensive Validation: Proper train/validation split and monitoring\n",
        "\n",
        "Robust Predictions: Handling edge cases and ensuring non-negative values\n",
        "\n",
        "Extensible Design: Easy to modify for ensemble approaches\n",
        "\n",
        "Next Steps for Improvement:\n",
        "Hyperparameter Tuning: Optimize learning rates, architecture\n",
        "\n",
        "Ensemble Methods: Combine multiple models\n",
        "\n",
        "Additional Features: Incorporate NDVI and height data if available\n",
        "\n",
        "Advanced Augmentation: More sophisticated image transformations\n",
        "\n",
        "Cross-Validation: More robust validation strategy\n",
        "\n",
        "This solution provides a solid foundation for predicting pasture biomass components and can be further optimized based on validation performance.'''"
      ],
      "metadata": {
        "id": "vtaTGOfB4axE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}